[{"body":"","excerpt":"","ref":"/docs/components/inputs/","title":"Inputs"},{"body":" Read the API documentation »\n Input KCL Overview This input fetches records from Kinesis with KCL. It consumes a specified stream, and processes all shards in that stream. It never exits. Multiple baker instances can consume the same stream, in that case the KCL will take care of balancing the shards between workers. Careful (shard stealing is not implemented yet). Resharding on the producer side is automatically handled by the KCL that will distribute the shards among KCL workers.\nConfiguration Keys available in the [input.config] section:\n   Name Type Default Required Description     AwsRegion string “us-west-2” false AWS region to connect to   Stream string \"\" true Name of Kinesis stream   AppName string \"\" true Used by KCL to allow multiple app to consume the same stream.   MaxShards int 32767 false Max shards this Worker can handle at a time   ShardSync duration 60s false Time between tasks to sync leases and Kinesis shards   InitialPosition string “LATEST” false Position in the stream where a new application should start from. Values: LATEST or TRIM_HORIZON    ","excerpt":" Read the API documentation »\n Input KCL Overview This input fetches …","ref":"/docs/components/inputs/kcl/","title":"KCL"},{"body":" Read the API documentation »\n Input Kinesis Overview This input fetches log lines from Kinesis. It listens on a specified stream, and processes all the shards in that stream. It never exits.\nConfiguration Keys available in the [input.config] section:\n   Name Type Default Required Description     AwsRegion string “us-west-2” false AWS region to connect to   Stream string \"\" true Stream name on Kinesis   IdleTime duration 100ms false Time between polls of each shard    ","excerpt":" Read the API documentation »\n Input Kinesis Overview This input …","ref":"/docs/components/inputs/kinesis/","title":"Kinesis"},{"body":" Read the API documentation »\n Input List Overview This input fetches logs from a predefined list of local or remote sources. The “Files” configuration variable is a list of “file specifiers”. Each “file specifier” can be:\n A local file path on the filesystem: the log file at that path will be processed A HTTP/HTTPS URL: the log file at that URL will be downloaded and processed A S3 URL: the log file at that URL that will be downloaded and processed “@” followed by a local path pointing to a file: the file is expected to be a text file and each line will be read and parsed as a “file specifier” “@” followed by a HTTP/HTTPS URL: the text file pointed by the URL will be downloaded, and each line will be read and parsed as a “file specifier” “@” followed by a S3 URL pointing to a file: the text file pointed by the URL will be downloaded, and each line will be read and parsed as a “file specifier” “@” followed by a local path pointing to a directory (must end with a slash): the directory will be recursively walked, and all files matching the “MatchPath” option regexp will be processed as logfiles “@” followed by a S3 URL pointing to a directory: the directory on S3 will be recursively walked, and all files matching the “MatchPath” option regexp will be processed as logfiles “-\": the contents of a log file will be read from stdin and processed “@-\": each line read from stdin will be parsed as a “file specifier”  All records produced by this input contain 2 metadata values:\n url: the files that originally contained the record last_modified: the last modification datetime of the above file  Configuration Keys available in the [input.config] section:\n   Name Type Default Required Description     Files array of strings [\"-\"] false List of log-files, directories and/or list-files to process   MatchPath string “.*.log.gz” false regexp to filter files in specified directories   Region string “us-west-2” false AWS Region for fetching from S3    ","excerpt":" Read the API documentation »\n Input List Overview This input fetches …","ref":"/docs/components/inputs/list/","title":"List"},{"body":" Read the API documentation »\n Input SQS Overview This input listens on multiple SQS queues for new incoming log files on S3; it is meant to be used with SQS queues popoulated by SNS. It never exits.\nConfiguration Keys available in the [input.config] section:\n   Name Type Default Required Description     AwsRegion string “us-west-2” false AWS region to connect to   Bucket string \"\" false S3 Bucket to use for processing   QueuePrefixes array of strings [] true Prefixes of the names of the SQS queues to monitor   MessageFormat string “sns” false The format of the SQS messages.   ‘plain’ the SQS messages received have the S3 file path as a plain string.       ‘sns’ the SQS messages were produced by a SNS notification.       FilePathFilter string \"\" false If provided, will only use S3 files with the given path.    ","excerpt":" Read the API documentation »\n Input SQS Overview This input listens …","ref":"/docs/components/inputs/sqs/","title":"SQS"},{"body":" Read the API documentation »\n Input TCP Overview This input relies on a TCP connection to receive records in the usual format Configure it with a host and port that you want to accept connection from. By default it listens on port 6000 for any connection It never exits.\nConfiguration Keys available in the [input.config] section:\n   Name Type Default Required Description     Listener string \"\" false Host:Port to bind to    ","excerpt":" Read the API documentation »\n Input TCP Overview This input relies on …","ref":"/docs/components/inputs/tcp/","title":"TCP"},{"body":"","excerpt":"","ref":"/docs/components/filters/","title":"Filters"},{"body":" Read the API documentation »\n Filter ClauseFilter Overview Discard records which do not match a clause given as a boolean S-expression. Check the filter documentation for some examples.\nClauseFilter boolean expression format This document describes the s-expression format used in ClauseFilter.\nThe format uses s-expressions. Empty string matches anything (i.e. all records will pass the expression).\nThere are only three keywords: and, or, not\nIf an s-expression starts with any other name, it is assumed to be the name of a field and it should be paired with the desired value to match against.\nMust match both X and Y to pass: (and X Y) You can use more than 2 arguments: (and X Y Z A B C) Must match either X or Y to pass: (or X Y) Must NOT match X to pass: (not X) Field must equal value to pass: (FIELD VALUE) example: (fieldName somevalue) Matches anything (because only one argument) (and X) Matches nothing (and) Matches anything (or)  Examples:\n(and (fieldName value1) (anotherFieldName value2)) (or (fieldName value1) (fieldName value2)) (not (or (fieldName value1) (fieldName value2))) (or (and (fieldName value1) (anotherFieldName value3)) (and (fieldName value2) (anotherFieldName value4)))  Configuration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Clause string \"\" false Boolean formula describing which events to let through. If empty, let everything through.    ","excerpt":" Read the API documentation »\n Filter ClauseFilter Overview Discard …","ref":"/docs/components/filters/clausefilter/","title":"ClauseFilter"},{"body":" Read the API documentation »\n Filter ClearFields Overview Reset a set of fields of all records passing through\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] true set of fields to clear    ","excerpt":" Read the API documentation »\n Filter ClearFields Overview Reset a set …","ref":"/docs/components/filters/clearfields/","title":"ClearFields"},{"body":" Read the API documentation »\n Filter Concatenate Overview Concatenate up to 10 fields' values to a single field\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] false The field names to concatenate, in order   Target string \"\" false The field name to save the concatenated value to   Separator string \"\" false Separator to concatenate the values. Must either be empty or a single ASCII, non-nil char    ","excerpt":" Read the API documentation »\n Filter Concatenate Overview Concatenate …","ref":"/docs/components/filters/concatenate/","title":"Concatenate"},{"body":" Read the API documentation »\n Filter Dedup Overview This filter removes duplicate records. A record is considered a duplicate, and is thus removed by this filter, if another record with the same values has already been seen. The comparison is performed on a user-provided list of fields (Fields setting).\nWARNING: to remove duplicates, this filter stores one key per unique record in memory, this means that the overall memory grows linearly with the number of unique records in your data set. Depending on your data set, this might lead to OOM (i.e. out of memory) errors.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] true fields to consider when comparing records   KeySeparator string “\\x1e” false character separator used to build a key from the fields    ","excerpt":" Read the API documentation »\n Filter Dedup Overview This filter …","ref":"/docs/components/filters/dedup/","title":"Dedup"},{"body":" Read the API documentation »\n Filter ExpandJSON Overview ExpandJSON extracts values from a JSON formatted record field and writes them into other fields of the same record. It supports JMESPath to select the values to copy inside the JSON.\nExample A possible filter configuration is:\n[[filter]] name=\"ExpandJSON\" [filter.config] Source = \"json_data\" [filter.config.Fields] jfield1 = \"field1\" jfield2 = \"field2\"  In this example, the filter extracts values of the jfield1 and jfield2 keys of the JSON object present in field json_dataof the record. Then, the values of that keys will be written into the field field1 and field2 of the same record.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Source string \"\" true record field that contains the json   Fields map of strings to strings  true \u003cJMESPath -\u003e record field\u003e map, the rest will be ignored   TrueFalseValues array of strings [“true”, “false”] false bind the json boolean values to correstponding strings    ","excerpt":" Read the API documentation »\n Filter ExpandJSON Overview ExpandJSON …","ref":"/docs/components/filters/expandjson/","title":"ExpandJSON"},{"body":" Read the API documentation »\n Filter FormatTime Overview This filter formats and converts date/time strings from one format to another. It requires the source and destination field names along with 2 format strings, the first one indicates how to parse the input field while the second how to format it.\nThe source time parsing can fail if the time value does not match the provided format. In this situation the filter clears the destination field, thus the user can filter out those results with a NotNull filter.\nMost standard formats are supported out of the box and you can provide your own format string, see Go time layout.\nSupported time format are:\n ANSIC format: “Mon Jan _2 15:04:05 2006” UnixDate format: “Mon Jan _2 15:04:05 MST 2006” RubyDate format: “Mon Jan 02 15:04:05 -0700 2006” RFC822 format: “02 Jan 06 15:04 MST” RFC822Z that is RFC822 with numeric zone, format: “02 Jan 06 15:04 -0700” RFC850 format: “Monday, 02-Jan-06 15:04:05 MST” RFC1123 format: “Mon, 02 Jan 2006 15:04:05 MST” RFC1123Z that is RFC1123 with numeric zone, format: “Mon, 02 Jan 2006 15:04:05 -0700” RFC3339 format: “2006-01-02T15:04:05Z07:00” RFC3339Nano format: “2006-01-02T15:04:05.999999999Z07:00” unix unix epoch in seconds unixms unix epoch in milliseconds unixns unix epoch in nanoseconds  Configuration Keys available in the [filter.config] section:\n   Name Type Default Required Description     SrcField string \"\" true Field name of the input time   DstField string \"\" true Field name of the output time   SrcFormat string “UnixDate” false Format of the input time   DstFormat string “unixms” false Format of the output time    ","excerpt":" Read the API documentation »\n Filter FormatTime Overview This filter …","ref":"/docs/components/filters/formattime/","title":"FormatTime"},{"body":" Read the API documentation »\n Filter Hash Overview This filter hashes a field using a specified hash function and writes the value to another (or the same) field. In order to have control over the set of characters present, the hashed value can optionally be encoded.\nSupported hash functions:\n md5 sha256  Supported encodings:\n hex (hexadecimal encoding)  Configuration Keys available in the [filter.config] section:\n   Name Type Default Required Description     SrcField string \"\" true Name of the field to hash   DstField string \"\" true Name of the field to write the result to   Function string \"\" true Name of the hash function to use   Encoding string \"\" false Name of the encoding function to use    ","excerpt":" Read the API documentation »\n Filter Hash Overview This filter hashes …","ref":"/docs/components/filters/hash/","title":"Hash"},{"body":" Read the API documentation »\n Filter MetadataLastModified Overview Extract the “last modified” timestamp from the record Metadata and write it to the selected field.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     DstField string \"\" true Name of the field into which write the timestamp to    ","excerpt":" Read the API documentation »\n Filter MetadataLastModified Overview …","ref":"/docs/components/filters/metadatalastmodified/","title":"MetadataLastModified"},{"body":" Read the API documentation »\n Filter MetadataUrl Overview This filter looks for ‘url’ in records metadata and copies it into a field of your choice, see DstField. If it doesn’t find the ‘url’ in the metadata, this filter clear DstField.\nIf you wish to discard records without the ‘url’ metadata, you can add the NotNull filter after this one in your topology.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     DstField string \"\" true Name of the field into to write the url to (or to clear if there’s no url)    ","excerpt":" Read the API documentation »\n Filter MetadataUrl Overview This filter …","ref":"/docs/components/filters/metadataurl/","title":"MetadataUrl"},{"body":" Read the API documentation »\n Filter NotNull Overview Discard the records having null (i.e empty) fields.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] true Fields is the list of fields to check for null/empty values    ","excerpt":" Read the API documentation »\n Filter NotNull Overview Discard the …","ref":"/docs/components/filters/notnull/","title":"NotNull"},{"body":" Read the API documentation »\n Filter PartialClone Overview Copy a list of fields to a new record and process this new record, discarding the original one\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] true Fields that must be copied to the new line    ","excerpt":" Read the API documentation »\n Filter PartialClone Overview Copy a …","ref":"/docs/components/filters/partialclone/","title":"PartialClone"},{"body":" Read the API documentation »\n Filter RegexMatch Overview Discard a record if one or more fields don’t match the corresponding regular expressions\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Fields array of strings [] false list of fields to match with the corresponding regular expression in Regexs   Regexs array of strings [] false list of regular expression to match. Fields[0] must match Regexs[0], Fields[1] Regexs[1] and so on    ","excerpt":" Read the API documentation »\n Filter RegexMatch Overview Discard a …","ref":"/docs/components/filters/regexmatch/","title":"RegexMatch"},{"body":" Read the API documentation »\n Filter ReplaceFields Overview Copy a field value or a fixed value to another field. Can copy multiple fields.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     CopyFields array of strings [] false List of src, dst field pairs, for example [“srcField1”, “dstField1”, “srcField2”, “dstField2”]   ReplaceFields array of strings [] false List of field, value pairs, for example: [“Foo”, “dstField1”, “Bar”, “dstField2”]    ","excerpt":" Read the API documentation »\n Filter ReplaceFields Overview Copy a …","ref":"/docs/components/filters/replacefields/","title":"ReplaceFields"},{"body":" Read the API documentation »\n Filter SetStringFromURL Overview This filter looks for a set of strings in the URL metadata and sets a field with the found string. Discards the log lines if URL metadata doesn’t contain any of the given strings.\nOn Error: the input record is discarded.\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Field string \"\" true Name of the field to set to   Strings array of strings [] true Strings to look for in the URL. Discard records not containing any of them.    ","excerpt":" Read the API documentation »\n Filter SetStringFromURL Overview This …","ref":"/docs/components/filters/setstringfromurl/","title":"SetStringFromURL"},{"body":" Read the API documentation »\n Filter StringMatch Overview Discard records if a field matches any of the provided strings\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Field string \"\" true name of the field which value is used for string comparison   Strings array of strings [] true list of strings to match.   InvertMatch bool false false Invert the match outcome, so that records are discarded if they don’t match any of the strings    ","excerpt":" Read the API documentation »\n Filter StringMatch Overview Discard …","ref":"/docs/components/filters/stringmatch/","title":"StringMatch"},{"body":" Read the API documentation »\n Filter Timestamp Overview Sets a field to the Unix Epoch timestamp at which the record is processed\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     Field string \"\" true field to set to the unix Epoch timestamp    ","excerpt":" Read the API documentation »\n Filter Timestamp Overview Sets a field …","ref":"/docs/components/filters/timestamp/","title":"Timestamp"},{"body":" Read the API documentation »\n Filter TimestampRange Overview Discard records if the value of a field containing a timestamp is out of the given time range (i.e StartDateTime \u003c= value \u003c EndDateTime)\nConfiguration Keys available in the [filter.config] section:\n   Name Type Default Required Description     StartDatetime string “no bound” true Lower bound of the accepted time interval (inclusive, UTC) format:‘2006-01-31 15:04:05’. Also accepts ‘now’   EndDatetime string “no bound” true Upper bound of the accepted time interval (exclusive, UTC) format:‘2006-01-31 15:04:05’. Also accepts ‘now’   Field string \"\" true Name of the field containing the Unix EPOCH timestamp    ","excerpt":" Read the API documentation »\n Filter TimestampRange Overview Discard …","ref":"/docs/components/filters/timestamprange/","title":"TimestampRange"},{"body":"","excerpt":"","ref":"/docs/components/outputs/","title":"Outputs"},{"body":" Read the API documentation »\n Output DynamoDB Overview This is a non-raw output, it doesn’t receive whole records. Instead it receives a list of fields for each record (output.fields in TOML).\nThis output writes the filtered log lines to DynamoDB. It must be configured specifying the region, the table name, and the columns to write. Columns are specified using the syntax “t:name” where “t” is the type of the data, and “name” is the name of column. Supported types are: “n” - integers; “s” - strings. The first column (and field) must be the primary key.\nConfiguration Keys available in the [output.config] section:\n   Name Type Default Required Description     Regions array of strings us-west-2 false DynamoDB regions to connect to   Table string \"\" true Name of the table to modify   Columns array of strings [] false Table columns that correspond to each of the fields being written   FlushInterval duration 1s false Interval at which flush the data to DynamoDB even if we have not reached 25 records   MaxWritesPerSec int 0 false Maximum number of writes per second that DynamoDB can accept (0 for unlimited)   MaxBackoff duration 2m false Maximum retry/backoff time in case of errors before giving up    ","excerpt":" Read the API documentation »\n Output DynamoDB Overview This is a …","ref":"/docs/components/outputs/dynamodb/","title":"DynamoDB"},{"body":" Read the API documentation »\n Output FileWriter Overview This is a raw output, for each record it receives a buffer containing the serialized record, plus a list holding a set of fields (output.fields in TOML).\nThis output writes the records into compressed files in a directory. Files will be compressed using Gzip or Zstandard based on the filename extension in PathString. The file names can contain placeholders that are populated by the output (see the keys help below). When the special {{.Field0}} placeholder is used, then the user must specify the field name to use for replacement in the fields configuration list. The value of that field, extracted from each record, is used as replacement and, moreover, this also means that each created file will contain only records with that same value for the field. Note that, with this option, the FileWriter creates as many workers as the different values of the field, and each one of these workers concurrently writes to a different file.\nConfiguration Keys available in the [output.config] section:\n   Name Type Default Required Description     PathString string \"\" false Template to describe location of the output directory: supports .Year, .Month, .Day and .Rotation. Also .Field0 if a field name has been specified in the output’s fields list.   RotateInterval duration 60s false Time after which data will be rotated. If -1, it will not rotate until the end.   ZstdCompressionLevel int 3 false zstd compression level, ranging from 1 (best speed) to 19 (best compression).   ZstdWindowLog int 0 false Enable zstd long distance matching. Increase memory usage for both compressor/decompressor. If more than 27 the decompressor requires special treatment. 0:disabled.    ","excerpt":" Read the API documentation »\n Output FileWriter Overview This is a …","ref":"/docs/components/outputs/filewriter/","title":"FileWriter"},{"body":" Read the API documentation »\n Output Nop Overview This is a non-raw output, it doesn’t receive whole records. Instead it receives a list of fields for each record (output.fields in TOML).\nNo-operation output. This output simply drops all lines and does not write them anywhere.\nConfiguration No configuration available\n","excerpt":" Read the API documentation »\n Output Nop Overview This is a non-raw …","ref":"/docs/components/outputs/nop/","title":"Nop"},{"body":" Read the API documentation »\n Output OpLog Overview This is a non-raw output, it doesn’t receive whole records. Instead it receives a list of fields for each record (output.fields in TOML).\nThis output writes the filtered log lines into the current baker log, purely for development purpose.\nConfiguration No configuration available\n","excerpt":" Read the API documentation »\n Output OpLog Overview This is a non-raw …","ref":"/docs/components/outputs/oplog/","title":"OpLog"},{"body":" Read the API documentation »\n Output Stats Overview This is a raw output, for each record it receives a buffer containing the serialized record, plus a list holding a set of fields (output.fields in TOML).\nCompute various distributions of the records it receives and dumps that to CSV. It computes the distribution of record by size and the distribution of the values of certain fields\nConfiguration Keys available in the [output.config] section:\n   Name Type Default Required Description     CountEmptyFields bool false false Whether fields with empty values are counted or not   CSVPath string “stats.csv” false Path of the CSV file to create   TimestampField string \"\" true Name of a field containing a POSIX timestamp (in seconds) used to build the times stats    ","excerpt":" Read the API documentation »\n Output Stats Overview This is a raw …","ref":"/docs/components/outputs/stats/","title":"Stats"},{"body":" Read the API documentation »\n Output WebSocket Overview This is a non-raw output, it doesn’t receive whole records. Instead it receives a list of fields for each record (output.fields in TOML).\nThis output writes the filtered log lines into any connected WebSocket client.\nConfiguration No configuration available\n","excerpt":" Read the API documentation »\n Output WebSocket Overview This is a …","ref":"/docs/components/outputs/websocket/","title":"WebSocket"},{"body":"","excerpt":"","ref":"/docs/components/uploads/","title":"Uploads"},{"body":" Read the API documentation »\n Upload S3 Overview S3Uploader uploads files to a destination on S3 that is relative to SourceBasePath\nConfiguration Keys available in the [upload.config] section:\n   Name Type Default Required Description     SourceBasePath string “/tmp/baker/ologs/” false Base path used to consider the final S3 path.   Region string “us-east-1” false S3 region to upload to   Bucket string \"\" true S3 bucket to upload to   Prefix string “/” false Prefix on the destination bucket   StagingPath string “/tmp/baker/ologs/staging/” false Local staging area to copy files to before upload.   Retries int 3 false Number of retries before a failed upload   Concurrency int 5 false Number of concurrent workers   Interval duration 15s false Period at which the source path is scanned   ExitOnError bool false false Exit at first error, instead of logging all errors    ","excerpt":" Read the API documentation »\n Upload S3 Overview S3Uploader uploads …","ref":"/docs/components/uploads/s3/","title":"S3"},{"body":"A Baker pipeline is declared in a configuration file in TOML format. We use this file to:\n define the topology (i.e the list of components) of the pipeline we want to run configure each component setup general elements such as metrics  Configuration file Baker is configured using a TOML file, which content is processed by the NewConfigFromToml function.\nThe file has several sections, described below:\n   Section Required Content     [general] false General configuration   [metrics] false Metrics service configuration   [fields] false Array of record fields names   [validation] false Input record field validation   [[user]] false Array of user-defined configurations   [input] true Input component configuration   [filterchain] false Filter chain global configuration   [[filter]] false Array of filters configuration   [output] true Output component configuration   [upload] false Upload component configuration    General configuration The [general] section is used to configure the general behaviour of Baker.\n   Key Type Effect     dont_validate_fields bool Reports whether records validation is skipped (by not calling Components.Validate)    Fields configuration The name configuration in the [fields] section provides a declarative way to define the structure of the records processed by Baker, without asking the user to define the FieldByName and FieldName functions.\nnames is a list of strings declaring the names of the fields and their position in the record (that is inherited by the position of the name in the list).\nSo, to make an example:\n[fields] names = [\"foo\", \"bar\"] defines a structure of the records with two fields: foo as first element and bar as second.\nValidation configuration The [validation] section is an optional configuration that contains one or more field names each of which is associated with a regular expression. If the validation section is specified Baker automatically generates a validation function, which checks that each input record satisfies the provided regular expression. The record is discarded at the first field that doesn’t match its associated regular expression. The user could choose to not provide record validation at all or to implement a more sophisticated validation function using a Go function specified in the Components struct. However, the validation could not be present both in the TOML and in the Components.\nTo make an example:\n[validation] foo = \"^\\w+$\" bar = \"[0-9]+\" defines that foo field must be a not empty word and bar field must contain a number. In this case, a valid record could be:\n   foo bar     hello_world hello23    The regular expression reference could be found at golang.org/s/re2syntax\nComponents configuration Components sections are [input], [[filter]], [output] and [upload] and contain a name = \"\u003ccomponent name\u003e\" line and an optional config subsection (like [input.config]) to set specific configuration values to the selected component.\nComponents' specific configuration can be marked as required (within the component code). If a required config is missing, Baker won’t start.\nThis is a minimalist Baker configuration TOML, reading records from files (List), applying the TimestampRange filter and writing the output to DynamoDB, with some specific options:\n[input] name=\"List\" [input.config] files=[\"records.csv.gz\"] [[filter]] name=\"TimestampRange\" [filter.config] StartDatetime = \"2020-10-30 15:00:00\" EndDatetime = \"2020-11-01 00:00:00\" Field = \"timestamp\" [output] name=\"DynamoDB\" fields=[\"source\",\"timestamp\",\"user\"] [output.config] regions=[\"us-west-2\",\"us-east-1\"] table=\"MyTable\" columns=[\"s:Source\", \"n:Timestamp\", \"s:User\"] [input] selects the input component, or where to read the records from.\nIn this case, the List component is selected, which is a component that fetches files from a list of local or remote paths/URLs. [input.config] is where component-specific configuration can be specified, and in this case we simply provide the files option to List.\nNotice that List would accept http:// or even s3:// URLs there in addition to local paths,\nand some more (run ./Baker-bin -help List in the help example for more details).\n[filterchain] defines the configuration for the whole filter chain. Filter-specific configurations are provided by [[filter]] (see below). The only accepted configuration in [filterchain] is procs = \u003cint\u003e that defines the number of concurrent filter chains. The default value is 16.\n[[filter]] In TOML syntax, the double brackets indicates an array of sections.\nThis is where you declare the list of filters (i.e filter chain) to sequentially apply to your records. As other components, each filter may be followed by a [filter.config] section.\n[output] selects the output component; the output is where records that made it to the end of the filter chain without being discarded end up. In this case, the DynamoDB output is selected, and its configuration is specified in [output.config].\nIn the example topology above we don’t specify an [upload] section since the output doesn’t create files on the local filesystem, it makes queries to DynamoDB.\nThe fields option in the [output] section selects which fields of the record are sent to the output.\nIn fact, most pipelines don’t want to send the full records to the output, but they select a few important fields out of the many available fields.\nNotice that this is just a selection: it is up to the output component to decide how to physically serialize those fields. For instance, the DynamoDB component requires the user to specify an option called columns that specifies the name and the type of the column where the fields are written.\nMetrics configuration The [metrics] section allows to configure the monitoring solution to use. Currently, only datadog is supported.\nSee the dedicated page to learn how to configure DataDog metrics with Baker.\nUser defined configurations The baker.NewConfigFromToml function, used by Baker to parse the TOML configuration file, can be also used to add custom configurations to the TOML file (useful as Baker can be used as library in a more complex project).\nThis is an example of a TOML file defining also some of those user defined configurations (along with the input and output configurations):\n[input] name=\"random\" [output] name=\"recorder\" [[user]] name=\"MyConfiG\" [user.config] field1 = 1 field2 = \"hello!\" Using NewConfigFromToml is then possible to retrieve those configurations:\ncfg := strings.NewReader(toml) // toml is the content of the toml file  // myConfig contains the user-defined configurations we expect from the toml file type myConfig struct { Field1 int Field2 string } mycfg := myConfig{} // comp is the baker components configuration. // Here we use Inputs and Outputs in addition to User because // they are required configurations comp := baker.Components{ Inputs: []baker.InputDesc{inputtest.RandomDesc}, Outputs: []baker.OutputDesc{outputtest.RecorderDesc}, User: []baker.UserDesc{{Name: \"myconfig\", Config: \u0026mycfg}}, } // Use baker to parse and ingest the configuration file baker.NewConfigFromToml(cfg, comp) // Now mycfg has been populated with the user defined configurations: // myConfig{Field1: 1, Field2: \"hello!\"} // and can be used anywhere in the program More examples can be found in the dedicated test file.\nEnvironment variables replacement Baker supports environment variables replacement in the configuration file.\nUse ${ENV_VAR_NAME} or $ENV_VAR_NAME and the value in the file is replaced at runtime.\nNote that if the variable doesn’t exist, then an empty string is used for replacement.\n","excerpt":"A Baker pipeline is declared in a configuration file in TOML format. …","ref":"/docs/how-tos/pipeline_configuration/","title":"Pipeline configuration"},{"body":"Looking to use Baker and start building pipelines now? Great, let’s see what you need.\nBaker is written in Go. To use it you need to import the Baker module into your program.\nIn this page we describe the simplest way to use Baker. At the end of it, we recommend reading the Baker Core Concepts and then have a deep dive in the How-to pages.\nAdd Baker… …to a brand-new project To create a new Go project (using Go modules) and add Baker, these are the suggested steps:\nmkdir myProject cd myProject go mod init github.com/myUser/myProject go get github.com/AdRoll/baker …to an existing project If you are adding Baker to a project already configured to use Go modules, just type:\ncd myProject go get github.com/AdRoll/baker Build and run Baker Once Baker has been added to the project, let’s see how to use it, with a minimalistic example.\nThis code comes from the cli example. You can find more examples in the examples/ folder of the project.\npackage main import ( \"log\" \"github.com/AdRoll/baker\" \"github.com/AdRoll/baker/filter\" \"github.com/AdRoll/baker/input\" \"github.com/AdRoll/baker/output\" \"github.com/AdRoll/baker/upload\" ) func main() { // Add all available components  comp := baker.Components{ Inputs: input.All, Filters: filter.All, Outputs: output.All, Uploads: upload.All, } // run Baker  if err := baker.MainCLI(comp); err != nil { log.Fatal(err) } } To create the binary, just build it:\ncd myProject go build -o baker-example . Configuration In the example above we use baker.MainCLI, an utility function that hides a lot of commonly used setup and requires a TOML file as first command line parameter.\nFor details about how to configure Baker, read the dedicated page.\nA simple example, in this case coming from the basic example, is the following:\n[fields] names=[\"timestamp\", \"source\", \"target\"] [input] name = \"List\" [input.config] files=[\"testdata/input.csv.gz\"] [[filter]] name=\"ReplaceFields\" [filter.config] ReplaceFields=[\"replaced\", \"timestamp\"] [output] name = \"FileWriter\" procs=1 [output.config] PathString=\"./_out/output.csv.gz\" Run the program Running the program is as simple as it sounds, at this point:\n$ ./baker-example /path/to/configuration.toml INFO[0000] Initializing fn=NewFileWriter idx=0 INFO[0000] FileWriter ready to log idx=0 INFO[0000] begin reading f=compressedInput.parseFile fn=testdata/input.csv.gz INFO[0000] end f=compressedInput.parseFile fn=testdata/input.csv.gz INFO[0000] terminating f=List.Run INFO[0000] Rotating current= idx=0 INFO[0000] Rotated current= idx=0 INFO[0000] FileWriter Terminating idx=0 INFO[0000] fileWorker closing idx=0 Next steps Do you want to know more about Baker?\nYou can read Baker Core concepts or, if you prefer to jump straight into the code, you can browse the API reference.\nMore detailed examples can be found in the How-tos section and the available components are documented in the components pages.\n","excerpt":"Looking to use Baker and start building pipelines now? Great, let’s …","ref":"/docs/getting-started/","title":"Getting started"},{"body":"Baker has been designed with high performance in mind. Baker core, the part of the code base which distributes records among components and ties them together, is very high-quality Go code. Records are never copied, and a particular attention has been given to reduce the number of memory allocations as much as possible, so as to keep the garbage collector cost to a minimum.\nBaker is also battle-tested, since 2016 NextRoll has been running hundreds if not thousands of Baker pipelines, processing petabytes, daily.\nWe report in this page some practical examples of Baker performances we could measure in the NextRoll production environment.\nWithin NextRoll, Baker is often executed on AWS EC2 instances, and thus you find in this page many references to EC2 instance types (c5.2xlarge, m5.12xlarge, etc.).\nRead from S3 and write to local disk On an AWS EC2 instance of size c5.2xlarge, Baker can read zstandard records from S3, uncompress them and apply a basic filtering logic, compressing them back on local files using ~90% of capacity of each vCPU (8 in total) and ~3.5GB of RAM.\nIt reads and writes a total of 94 million records in less than 9 minutes, that’s 178k records per second.\nOn a c5.2xlarge instance (48 vCPUs) the same test takes 2 minutes, so that’s a speed of 775k records per second.\nFor this test we use 711 zstd compressed files for a total of 17 GB of compressed size and 374 GB of uncompressed size. The average size of each record is 4.5 KB.\nRead from S3 and write to DynamoDB (in the same region) On a c5.4xlarge instance, Baker reads zstd compressed files from S3 writing to DynamoDB (configured with 20k write capacity units) at an average speed of 60k records/s (the average size of each record is 4.3 KB) using less than 1 GB of memory and 300% of the total CPU capacity (less than 20% for each core).\nThe bottleneck here is the DynamoDB write capacity, so Baker could handle the additional load caused by a possible increase in the write capacity.\nRead from Kinesis and write to DynamoDB (in the same region) On a c5.4xlarge instance, we performed a test reading from a Kinesis stream with 130 shards and writing to a DynamoDB table with 20k write capacity units. Baker is able to read and write more than 10k records per second (the average size of each record is 4.5 KB) using less than 1 GB of RAM and around 400% of the total CPU capacity (each core being used at less than 25%).\n","excerpt":"Baker has been designed with high performance in mind. Baker core, the …","ref":"/docs/performance/","title":"Performance"},{"body":"Pipeline A pipeline (a.k.a. Topology) is the configured set of operations that Baker performs during its execution.\nIt is configured in a TOML file and is defined by:\n One input component, determining where to fetch records from Zero or more filters, applied sequentially, which together compose the filter chain. A filter is a function that processes record: it can modify fields, discard records or create additional ones One output component, specifying where to send the records that made it so far One optional upload component, that can be added if the output creates files that need to be uploaded to a remote destination  Notice that there are two main usage scenarios for Baker, batch or daemon processing, that depend on the input component behavior:\n Batch processing: In this case, Baker goes through all the records that are fed by the input component, processes them as quickly as possible, and exits when the input component ends its job. Daemon: in this case, the input component never exits and thus also Baker, that keeps waiting for incoming records from the input (e.g.: Kinesis), processes them and sends them to the output.  Also read Pipeline configuration\nRecord and LogLine Baker processes “records”. A Record is an interface that provides an abstraction over a record of flattened data, where columns of fields are indexed through integers.\nBaker currently provides a single implementation of Record, called LogLine ( API reference).\nIf LogLine doesn’t fit your needs, you can customize it or implement your version of the Record.\nComponents To process records, Baker uses up to 4 component types, each one with a different job:\n Input reads blobs of data representing serialized records and sends them to Baker. Baker then parses the raw bytes, creates records from them and sends them through the filter chain, an ordered list of Filter components that can modify, drop or create Records. At the end of the filter chain, records are sent to the Output component. There are 2 types of output components. Raw outputs receive serialized records while non-raw outputs just receive a set of fields. Whatever its type, the output most certainly writes records on disk or to an external service. In case the output saves files to disk, an optional Upload component can upload these files to a remote destination, such as Amazon S3 for example.  Read our How-to guides to know how to:\n create an Input component create a Filter component create an Output component create an Upload component  Metrics During execution, Baker collects different kind of performance data points:\n General pipeline metrics such as the total number of records processed and records per seconds. Component-specific metrics: files written per second, discarded records (by a filter), errors, etc. Go runtime metrics: mallocs, frees, garbage collections and so on.  If enabled, Baker collects all these metrics and publishes them to a monitoring solution, such as Datadog or Prometheus.\nMetrics export is configured in Baker topology TOML files, see how to configure it.\nBaker also prints general metrics once per second on standard output, in single-line format. Read more about it here.\nSharding Baker supports partitioning the records it processes into smaller subsets each of which is forwarded to an output shard: divide and conquer.\nWhen sharding is enabled, the shards, which are just multiple instances of the same output component, run concurrently. Each of them only gets to process a specific subset of records, based on a the value a specific field has. This horizontal partioning allows to get the most of the resources at your disposal, since you can perform more work at the same time.\nRead more about sharding and how to configure it\n","excerpt":"Pipeline A pipeline (a.k.a. Topology) is the configured set of …","ref":"/docs/core-concepts/","title":"Core concepts"},{"body":"Baker processes objects called records. A Record, in Baker, is an interface that provides an abstraction over a record of structured data, where fields are indexed and accessed via integers.\nAt the moment, Baker provides a single implementation of the Record interface, called LogLine.\nIf LogLine doesn’t fit your needs, you can implement the Record interface or modify LogLine. See the custom Record how-to for more details about implementing the Record interface from scratch.\nLogLine A LogLine is an implementation of the Record interface which is highly optimized for fast parsing and serializing of CSV records.\nIt supports any single-byte field separator and doesn’t handle quotes (neither single nor double).\nThe maximum number of fields is hard-coded by the LogLineNumFields constant which is 3000.\n100 extra fields can be stored at runtime in a LogLine (also hardcoded with NumFieldsBaker), these extra fields are a fast way to exchange data between filters and/or outputs but they are neither handled during Parsing (i.e LogLine.Parse) nor serialization (LogLine.ToText).\nCustom LogLine If the hardcoded values for LogLineNumFields and NumFieldsBaker do not suit your needs, you can copy logline.go in your project and modify the constants declared at the top of the file.\nYour specialized LogLine will still implement baker.Record and thus can be used in lieu of baker.LogLine.\nThe CreateRecord function set into baker.Components must return an instance of your custom LogLine instead of the default one.\n","excerpt":"Baker processes objects called records. A Record, in Baker, is an …","ref":"/docs/how-tos/record_and_logline/","title":"Record and LogLine"},{"body":"Inputs  KCL Kinesis List SQS TCP  Filters  ClauseFilter ClearFields Concatenate Dedup ExpandJSON FormatTime Hash MetadataLastModified MetadataUrl NotNull PartialClone RegexMatch ReplaceFields SetStringFromURL StringMatch Timestamp TimestampRange  Outputs  DynamoDB FileWriter Nop OpLog Stats WebSocket  Uploads  S3  ","excerpt":"Inputs  KCL Kinesis List SQS TCP  Filters  ClauseFilter ClearFields …","ref":"/docs/components/","title":"Components"},{"body":"To configure and run a Baker topology, 4 steps are required:\n use a TOML configuration file define a baker.Components object obtain a Baker configuration object calling baker.NewConfigFromToml run baker.Main  The example folder in the Baker repositories contains many examples of implementing a Baker pipeline.\nStart with the basic example.\n","excerpt":"To configure and run a Baker topology, 4 steps are required:\n use a …","ref":"/docs/how-tos/","title":"How-Tos"},{"body":"As you can read in the Record and LogLine page, Baker processes objects called records.\nA Record, in Baker, is an interface that provides an abstraction over a record of flattened data, where columns of fields are indexed through integers.\nIf the Record implementations provided by Baker doesn’t fit your needs, you can create your own version, implementing the Record inteface.\nHow to use a custom version of the Record Once your Record version is ready, you need to use it in your code.\nIn order to do so, you must create and fill a baker.Components struct.\ntype Components struct { Validate ValidationFunc CreateRecord func() Record FieldByName func(string) (FieldIndex, bool) FieldNames []string // ... other fields } Validate Validate is the function used to validate a record. It is called for each processed record unless not set or when the [general] dont_validate_fields = true configuration is set in the TOML file.\nRegardless of the TOML configuration, the function is passed to all components that can use it at their will.\nCreateRecord CreateRecord is the function that creates a new record. If not set, a default function is used that creates a LogLine with , as field separator.\nThe function is used internally by Baker to create new records every time a new one comes from the input.\nThe function is also passed to components that can use it to create new records while processing.\nFieldByName FieldByName gets a field index by its name. The function is mainly used by the components (that receive it during setup) to retrieve the index of a field they need for filtering or processing, but it is also used internally by Baker when sending fields to the output (when at least one field is selected in the output TOML configuration).\nFieldNames FieldNames []string lists the names of the fields a Record can have. Since it’s a slice, its length also indicates the maximum number of fields.\nRecord conformance test test_helper.go provides a test helper, RecordConformanceTest, one can and should use to verify their custom Record satisfies the invariants required for any Record implementation.\nJust pass to RecordConformanceTest a factory function creating new instances of your Record.\nWarning The conformance test provides a way to verify that a record implementation respects the invariant that Baker requires for a Record implementation and thus it should always be executed against all custom implementations of the Record.  ","excerpt":"As you can read in the Record and LogLine page, Baker processes …","ref":"/docs/how-tos/custom_record/","title":"Create a custom Record"},{"body":"","excerpt":"","ref":"/docs/tutorials/","title":"Tutorials"},{"body":"In this tutorial you’ll learn how to create a Baker-based program to process a given dataset (in CSV format), filter records based on your needs and save the result to S3.\nThe dataset we’re going to use is an open dataset containing ratings on many Ramens, the famous japanese noodle soup!\nOur goal is to discard all ramens that have never been on a top-ten ranking, split the results into multiple folders named after the ramens source countries, and upload the resulting lists to S3.\nThe dataset The dataset file has 7 columns:\n review_num: the number of the review (higher numbers mean more recent reviews) brand: the name of the restaurant variety: the name of the recipe style: the type of the ramen (cup, pack, bowl, etc) country: self-explanatory stars: ratings stars (from 0 to 5) top_ten: whether the ramen has been included in a top-ten ranking  Warning The original CSV file can’t be immediately used with Baker because:\n it includes a header row some fields have values with commas and thus are enclosed in double-quotes. Baker doesn’t support it the file is uncompressed  For the purpose of this tutorial we’ve already prepared the final file for you and it is available for downloading here.\n The required components  List: reads the input file from disk NotNull: discards all ramens without a top-ten entry FileWriter: saves the resulting file to disk S3: uploads the file to S3  Baker configuration An essential thing to do is to create a configuration file for Baker, in TOML format, selecting the aforementioned components:\n[fields] names = [\"review_num\", \"brand\", \"variety\", \"style\", \"country\", \"stars\", \"top_ten\"] [input] name = \"List\" [input.config] Files = [\"/tmp/db.csv.gz\"] # put the file wherever you like [[filter]] name = \"NotNull\" [filter.config] Fields = [\"top_ten\"] # discard all records with an empty top_ten field [output] name = \"FileWriter\" procs = 1 # With our PathString, FileWriter doesn't support concurrency fields = [\"country\"] [output.config] PathString = \"/tmp/out/{{.Field0}}/ramens.csv.gz\" [upload] name=\"S3\" [upload.config] Region = \"us-east-1\" Bucket = \"myBucket\" Prefix = \"ramens/\" StagingPath = \"/tmp/staging/\" SourceBasePath = \"/tmp/out/\" Interval = \"60s\" ExitOnError = true Create the program Baker is a Go library. To use it, it is required to create a Go main() function, define a baker.Components object and pass it to baker.MainCLI():\npackage main import ( \"log\" \"github.com/AdRoll/baker\" ) func main() { components := baker.Components{/* define components */} if err := baker.MainCLI(components); err != nil { log.Fatal(err) } } Define baker.Components The only required fields in baker.Components are the components that we need to use (the complete guide to baker.Components is here).\nThe simplest and more generic way to add the components to Baker is to add all of them:\ncomponents := baker.Components{ Inputs: input.All, Filters: filter.All, Outputs: output.All, Uploads: upload.All, } The complete program (that is available in the tutorials/ folder in the Baker repository) is the following:\npackage main import ( \"log\" \"github.com/AdRoll/baker\" \"github.com/AdRoll/baker/input\" \"github.com/AdRoll/baker/filter\" \"github.com/AdRoll/baker/output\" \"github.com/AdRoll/baker/upload\" ) func main() { if err := baker.MainCLI(baker.Components{ Inputs: input.All, Filters: filter.All, Outputs: output.All, Uploads: upload.All, }); err != nil { log.Fatal(err) } } Run the program Once the code and the configuration files are ready, we can run the topology:\n$ go build -o myProgram ./main.go # Test it works as expected $ ./myProgram -help # run the topology $ ./myProgram topology.toml Among the messages that Baker prints on stdout, the stats messages are particularly interesting:\nStats: 1s[w:0 r:0] total[w:41 r:2584 u:11] speed[w:20 r:1292] errors[p:0 i:0 f:2543 o:0 u:0] Take a look at the dedicated page to learn how to read the values.\nVerify the result The resulting files are split into multiple folders, one for each country, and then uploaded.\nThe S3 upload removes the files from the local disk once uploaded, so you’ll only find empty directories in the output destination folder:\n~ ls -l /tmp/out/ drwxrwxr-x - username 16 Nov 11:43 China drwxrwxr-x - username 16 Nov 11:43 Hong Kong drwxrwxr-x - username 16 Nov 11:43 Indonesia drwxrwxr-x - username 16 Nov 11:43 Japan drwxrwxr-x - username 16 Nov 11:43 Malaysia drwxrwxr-x - username 16 Nov 11:43 Myanmar drwxrwxr-x - username 16 Nov 11:43 Singapore drwxrwxr-x - username 16 Nov 11:43 South Korea drwxrwxr-x - username 16 Nov 11:43 Taiwan drwxrwxr-x - username 16 Nov 11:43 Thailand drwxrwxr-x - username 16 Nov 11:43 USA The files have been uploaded to S3:\n~ aws s3 ls --recursive s3://myBucket/ramens/ 2020-11-16 11:43:59 115 ramens/China/ramens.csv.gz 2020-11-16 11:43:59 83 ramens/Hong Kong/ramens.csv.gz 2020-11-16 11:43:59 223 ramens/Indonesia/ramens.csv.gz 2020-11-16 11:43:59 236 ramens/Japan/ramens.csv.gz 2020-11-16 11:43:59 240 ramens/Malaysia/ramens.csv.gz 2020-11-16 11:43:59 99 ramens/Myanmar/ramens.csv.gz 2020-11-16 11:43:59 219 ramens/Singapore/ramens.csv.gz 2020-11-16 11:43:59 265 ramens/South Korea/ramens.csv.gz 2020-11-16 11:43:59 159 ramens/Taiwan/ramens.csv.gz 2020-11-16 11:43:59 181 ramens/Thailand/ramens.csv.gz 2020-11-16 11:43:59 94 ramens/USA/ramens.csv.gz Conclusion This is it for this basic tutorial. You have learned:\n how to create a simple Baker program to process a CSV dataset with minimal filtering and upload the results to S3 how to create the Baker TOML configuration file how to execute the program and verify the result  You can now improve your Baker knowledge by taking a look at the other tutorials and learning more advanced topics.\n","excerpt":"In this tutorial you’ll learn how to create a Baker-based program to …","ref":"/docs/tutorials/basic/","title":"Basic: build a simple pipeline"},{"body":"The baker.Components struct lists all the components available to Baker when defining topologies.\nHence, to create a topology, Baker requires:\n an instance of baker.Components a TOML configuration file describing the topology we want to run  func main() { comp := baker.Component { // ... \t} f, _ := os.Open(\"/path/to/topology.go\") cfg, _ := baker.NewConfigFromToml(f, components) _ = baker.Main(cfg) } The next paragraphs give you a high level overview of each field of the baker.Components struct.\nInputs, Filters, Outputs and Uploads These fields list the components that are available to topologies. All components present in baker.Components can be used in the TOML configuration file.\nThe following is an example of baker.Components where:\n we use all inputs and uploads provided in Baker repository only a single filter is set, a custom one we declared ourselves all Baker outputs are added in addition our own custom output  import ( \"github.com/AdRoll/baker\" \"github.com/AdRoll/baker/input\" \"github.com/AdRoll/baker/output\" \"github.com/AdRoll/baker/upload\" ) comp := baker.Components{ Inputs: input.All, Filters: []baker.FilterDesc{MyCustomFilterDesc}, Outputs: append(output.All, MyCustomOutputDesc...), Uploads: upload.All, // Other fields not shown here. } Metrics Metrics lists the metrics clients available when creating topologies.\nimport ( \"github.com/AdRoll/baker\" \"github.com/AdRoll/baker/metrics\" ) comp := baker.Components{ Metrics: metrics.All, // Other fields not shown here. } This list can contain a metric backend already included into Baker or a custom implementation of the baker.MetricsClient interface.\nFor more, see the page dedicated to metrics.\nUser import \"github.com/AdRoll/baker\" comp := baker.Components{ User: []baker.UserDesc{ /* list of user-specific structs */}, // Other fields not shown here. } Baker users might want to use Baker TOML files to store application-specific configuration. The User field lists user-defined configurations structures which aren’t strictly useful to Baker.\nTo learn more about this topic, read the dedicated section in the Pipeline configuration page.\nShardingFuncs import \"github.com/AdRoll/baker\" shardingFuncs := make(map[baker.FieldIndex]baker.ShardingFunc) comp := baker.Components{ ShardingFuncs: shardingFuncs, // Other fields not shown here. } ShardingFuncs holds a dictionary associating field indexes to hash functions. When sharding is enabled, these hash functions are used to determine which shard a record is sent to.\nValidate import \"github.com/AdRoll/baker\" func validate(baker.Record) (bool, baker.FieldIndex) { // ... } comp := baker.Components{ Validate: validate, // Other fields not shown here. } Validate is the function used to validate a record. It is called for each processed record unless nil or when dont_validate_fields is set to true in TOML’s [general] section.\nRegardless of the dont_validate_fields value, the Validate function is made accessible to all components so that they can use it at their will.\nA simple validation function based on regular expression could be enabled from the [validation] section of the TOML. Anyways, the user should specify the validation either in the Components or in the TOML.\nCreateRecord import \"github.com/AdRoll/baker\" func create() baker.Record { // ... } comp := baker.Components{ CreateRecord: create, // Other fields not shown here. } CreateRecord is a factory function returning new Record instances. If not set, a default function is used that creates a LogLine with the comma field separator.\nThe function is used internally by Baker each time a new Record must be created. This happens when blobs of raw serialized data, provided by the Input component, are parsed.\nThe function is also available for components needing to create new records.\nFieldByName import \"github.com/AdRoll/baker\" func fieldByName(name string) (baker.FieldIndex, bool) { // ... } comp := baker.Components{ FieldByName: fieldByName, // Other fields not shown here. } FieldByName returns the index of a field given its name.\nInternally Baker refers to fields by their indices, but it’s simpler for users to refer to fields with their names. This function exists to convert a field name to its index, it also controls if the name is valid.\nThe function is mainly used by the components (that receive it during setup) to retrieve the index of a field they need for filtering or processing, but it is also used internally by Baker when sending fields to the output (when at least one field is selected in the output TOML configuration).\nFieldNames import \"github.com/AdRoll/baker\" fieldNames := []string{\"field0\", \"field1\", \"field2\", \"field3\"} comp := baker.Components{ FieldNames: fieldNames, // Other fields not shown here. } FieldNames is the slice holding the record field names.\nRecord fields are 0-based indices, and thus the role of the FieldNames slice is twofold: first it allows Baker components to refer to a record field by its name rather than its index. It also set an upper-bound on the number of declared fields a Record can have, which is useful in some cases. That’s why FieldNames is provided for components to use in case they need it.\nIf the FieldNames slice has not been set, Baker generates it automatically from the [fields] section in Baker TOML configuration file. However Baker will refuse to start if field names are neither set in baker.Components nor in the configuration file.\n","excerpt":"The baker.Components struct lists all the components available to …","ref":"/docs/how-tos/baker_components/","title":"baker.Components"},{"body":"The job of a Baker input is to fetch blob of data containing one or multiple serialized records and send them to Baker.\nThe input isn’t in charge of splitting/parsing the input data into Records (that is done by Baker), but only retrieving them as fast as possible in raw format adding, if any, metadata to them and then sending those values to Baker through a *Data channel. The channel size is customizable in the topology TOML with [input] chansize=\u003cvalue\u003e (default to 1024).\nTo create an input and make it available to Baker, one must:\n Implement the Input interface Fill an InputDesc structure and register it within Baker via Components.  Daemon vs Batch The input component determines the Baker behavior between a batch processor or a long-living daemon.\nIf the input exits when its data processing has completed, then Baker waits for the topology to end and then exits.\nIf the input never exits, then Baker acts as a daemon.\nData The Data object that the input must fill in with read data has two fields: Bytes, that must contain the raw read bytes (possibly containing more records separated by \\n), and Meta.\nMetadata can contain additional information Baker will associate with each of the serialized Record contained in Data.\nTypical information could be the time of retrieval, the filename (in case Records come from a file), etc.\nThe Input interface New Input components need to implement the Input interface.\ntype Input interface { Run(output chan\u003c- *Data) error Stop() Stats() InputStats FreeMem(data *Data) } The Run function implements the component logic and receives a channel where it sends the raw data it processes.\nFreeMem(data *Data) is called by Baker when data is no longer needed. This is an occasion for the input to recycle memory, for example if the input uses a sync.Pool to create new instances of baker.Data.\nInputDesc var MyInputDesc = baker.InputDesc{ Name: \"MyInput\", New: NewMyInput, Config: \u0026MyInputConfig{}, Help: \"High-level description of MyInput\", } This object has a Name, that is used in the Baker configuration file to identify the input, a costructor-like function (New), a config object (where the parsed input configuration from the TOML file is stored) and a help text that must help the users to use the component and its configuration parameters.\nThe New function The New field in the InputDesc object should be to assigned to a function that returns a new Input.\nThe function receives a InputParams object and returns an instance of Input.\nThe function should verify the configuration params into InputParams.DecodedConfig and initialize the component.\nInput configuration and help The input configuration object (MyInputConfig in the previous example) must export all configuration parameters that the user can set in the TOML topology file.\nEach field in the struct must include a help string tag (mandatory) and a required boolean tag (default to false).\nAll these parameters appear in the generated help. help should describe the parameter role and/or its possible values, required informs Baker it should refuse configurations in which that field is not defined.\nWrite tests To test an input component we suggest two main paths:\n test the component in isolation, calling the Run function write an higher-level test by running a complete Baker topology  Regardless of the chosen path, two additional unit tests are always suggested:\n test the New() (constructor-like) function, to check that the function is able to correctly instantiate the component with valid configurations and intercept incorrect ones (in case that’s possible) create small and isolated functions where possible and unit-test them  Test calling Run() In case we want to test the component calling the Run function, this is an example of test where, after some initialization, the input.Run function is called and the produced Data is checked in a goroutine:\nfunc TestMyInput(t *testing.T) { ch := make(chan *baker.Data) defer close(ch) // start a goroutine that acts as Baker, consuming the baker.Data produced by the input  go func() { for data := range ch { // test `data`, that comes from the component,  // like checking its content, parse the records, metadata, etc  if something_is_wrong(data) { t.Fatalf(\"error!\") } } }() // Configure the input.  cfg := ... input, err := NewMyInput(cfg) // use the contructor-like New function  // check err  // run the input  if err := input.Run(ch); err != nil { t.Fatal(err) } } The List input has an example of this testing strategy (look for the TestListBasic test).\nTest the component running a topology If we want to test the component creating and running a topology, we need to create one starting from the TOML configuration and then calling NewConfigFromToml, NewTopologyFromConfig and Run.\nThe Base, Recorder and RawRecorder outputs included in the outputtest package can be helpful here to obtain the output and check it:\nfunc TestMyInput(t *testing.T) { toml := ` [input] name = \"MyInput\" [output] name=\"RawRecorder\" procs=1 ` // Add the input to be tested and a testing output  c := baker.Components{ Inputs: []baker.InputDesc{MyInputDesc}, Outputs: []baker.OutputDesc{outputtest.RawRecorderDesc}, } // Create and start the topology  cfg, err := baker.NewConfigFromToml(strings.NewReader(toml), c) if err != nil { t.Error(err) } topology, err := baker.NewTopologyFromConfig(cfg) if err != nil { t.Error(err) } topology.Start() // In this goroutine we should provide some inputs to the component  // The format and how to send them to the component, depends on  // the component itself  go func() { defer topology.Stop() sendDataToMyInput() // fake function, you need to implement your logic here  } topology.Wait() // wait for Baker to quit after `topology.Stop()`  if err := topology.Error(); err != nil { t.Fatalf(\"topology error: %v\", err) } // retrieve the output and test the records  out := topology.Output[0].(*outputtest.Recorder) if len(out.Records) != want { t.Errorf(\"want %d log lines, got %d\", want, len(out.Records)) } // more testing on out.Records... } The TCP input includes an example of this testing strategy.\n","excerpt":"The job of a Baker input is to fetch blob of data containing one or …","ref":"/docs/how-tos/create_input/","title":"Create a custom input component"},{"body":"Creating a custom filter is probably the most common action a Baker user will perform.\nIn fact, filters are the components that apply the business logic to a Baker pipeline, creating or discarding records or modifying fields.\nA working example of a custom filter can be found in the filtering example\nTo create a filter and make it available to Baker, one must:\n Implement the Filter interface Add a FilterDesc for the filter to the available filters in Components  The Filter interface New Filter components need to implement the Filter interface.\ntype Filter interface { Process(l Record, next func(Record)) Stats() FilterStats }  Process is the function that actually filters the records Stats return statistics (FilterStats) about the filtering process  A very simple example of filter doing nothing is:\ntype MyFilter struct{ numProcessedLines int64 } func (f *MyFilter) Process(r baker.Record, next func(baker.Record)) { atomic.AddInt64(\u0026f.numProcessedLines, 1) next(r) } func (f *MyFilter) Stats() baker.FilterStats { return baker.FilterStats{ NumProcessedLines: atomic.LoadInt64(\u0026f.numProcessedLines), } } FilterDesc To be included in the Baker filters, a filter must be described by a FilterDesc object:\nvar MyFilterDesc = baker.FilterDesc{ Name: \"MyFilter\", New: NewMyFilter, Config: \u0026MyFilterConfig{}, Help: \"This filter does nothing, but in a great way!\", } This object has a Name, that is used in the Baker configuration file to identify the filter, a costructor function (New), a config object (used to parse the filter configuration in the TOML file) and a help text.\nIn this case the filter can be used with this configuration in the TOML file:\n[[filter]] name = \"MyFilter\" The New function The New field in the FilterDesc object should be to assigned to a function that returns a new Filter.\nEach filter must have a constructor function that receives a FilterParams and returns the Filter interface implemented by the filter:\nfunc NewMyFilter(cfg baker.FilterParams) (baker.Filter, error) { return \u0026MyFilter{}, nil } The filtering example shows a more complex constructor that also uses the FilterParams argument.\nFilter configuration and help A filter requiring some configurations also has a config object, including as many keys as it needs and tagging each one with an help tag, a string that contains what a user needs to know which values set for it:\ntype ClauseFilterConfig struct { Clause string `help:\"Boolean formula describing which events to let through. If empty, let everything through.\"` } Other available tags are required:\"true|false\" and default:\"\u003cvalue\u003e\".\nModify record fields A filter can change the value of the record fields before calling next():\nfunc (f *MyFilter) Process(r Record, next func(Record)) { var src FieldIndex = 10 var dst FieldIndex = 10 v := r.Get(src) //.. modify v as required  r.Set(dst, v) next(r) } Processing records Filters do their work in the Process(r Record, next func(Record) method, where r is the Record to process and next is a closure assigned to the next element in thefilter chain.\nFilters call next(r) once they’re done with the record and desire to forward it, or simply do not call next() if they want to discard the record.\nWhen a filter discards a record it should also report it in the stats:\ntype MyFilter struct{ numProcessedLines int64 numFilteredLines int64 } func (f *MyFilter) Process(r Record, next func(Record)) { atomic.AddInt64(\u0026f.numProcessedLines, 1) // shouldBeDiscarded is part of the filter logic  if shouldBeDiscarded(r) { atomic.AddInt64(\u0026f.numFilteredLines, 1) // return here so next() isn't called  return } // forward the record to the next element of the filter chain  next(r) } func (f *MyFilter) Stats() FilterStats { return baker.FilterStats{ NumProcessedLines: atomic.LoadInt64(\u0026f.numProcessedLines), NumFilteredLines: atomic.LoadInt64(\u0026f.numFilteredLines), } } Create records A filter can decide to call next() multiple times to send new or duplicated records to the next element of the filter chain.\nNote that the new or copied records don’t start the filter chain from the first filter in the list but only the remaining filters are applied to the records.\nRemember not to pass the same record to multiple next() functions or later changes to one of the records could also impact the others.\nAlways use Copy() or CreateRecord() before calling next() more than once.  Copy() Filters can duplicate incoming records (with record.Copy()), and thus have more records come out than records that came in.\nfunc (f *MyFilter) Process(r Record, next func(Record)) { // Call next the 1st time  next(r) // WRONG, it is the same record as above  next(r) // CORRECT, this is a copy of the record  next(r.Copy()) } CreateRecord() A new, empty, record is created calling the CreateRecord function. The CreateRecord function is available as part of the FilterParams argument of the filter constructor. If you plan to use it in the Process function then store it to the filter object in the constructor as shown in this example:\ntype MyFilter struct{ cfg baker.FilterParams } func NewMyFilter(cfg baker.FilterParams) (baker.Filter, error) { return \u0026MyFilter{ cfg: cfg, // you can also store only CreateRecord  }, nil } func (f *MyFilter) Process(r Record, next func(Record)) { newRecord := f.cfg.CreateRecord() //... do something with the record  next(newRecord) } Write tests When writing tests for a new filter, particular attention should be given to:\n the New() (constructor-like) function the Process() function  Testing the New function means testing that we’re able to intercept wrong configurations.\nAn example, using the NewMyFilter function, is:\ncfg := baker.FilterParams{ ComponentParams: baker.ComponentParams{ DecodedConfig: \u0026MyFilterConfig{}, }, } if filter, err := NewMyFilter(cfg); err != nil { t.Errorf(\"unexpected error: %v\", err) }  Obviously, if the filter requires some configuration values (not like this empty demo filter), the test should also verify all possible values and corner cases.\n With the filter instance, it’s then possible to test the Process() function, providing a manually crafted Record and checking whether the function calls the next() function:\nll := \u0026baker.LogLine{FieldSeparator: ','} // Set values to the record, triggering all the filter logic // ll.Set(\u003csomeIndex\u003e, []byte(\"somevalue\")) kept := false filter.Process(ll, func(baker.Record) { kept = true }) // check `kept` depending on what is expected for the set values ","excerpt":"Creating a custom filter is probably the most common action a Baker …","ref":"/docs/how-tos/create_filter/","title":"Create a custom filter component"},{"body":"Output components in Baker receive records at the end of the filter chain and are in charge of storing them, eventually sending the result (like a temporary file in the disk) to an Upload component.\nTo create an output and make it available to Baker, one must:\n Implement the Output interface Fill-up an OutputDesc struct and register it within Baker via Components  The Output interface New Output components need to implement the Output interface.\ntype Output interface { Run(in \u003c-chan OutputRecord, upch chan\u003c- string) error Stats() OutputStats CanShard() bool } The Run function implements the component logic and gets a channel where it receives OutputRecord objects and a channel to communicate to the Upload components what to upload.\nCanShard is the function telling whether the output is able to manage sharding. Read the page dedicated to the sharding to go deeper in the topic.\nStats is used to report metrics, see the dedicated page.\nOutputDesc var MyOutputDesc = baker.OutputDesc{ Name: \"MyOutput\", New: NewMyOutput, Config: \u0026MyOutputConfig{}, Raw: true, Help: \"High-level description of MyOutput\", } This object has a Name, that is used in the Baker configuration file to identify the output, a constructor-like function (New), a config object (used to parse the output configuration in the TOML file) and a help text that must help the users to use the component and its configuration parameters. The Raw field instructs Baker whether it should send raw records in addition to single fields (see below for details).\nThe New function The New field in the OutputDesc object should be to assigned to a function that returns a new Output.\nThe function receives an OutputParams object and returns an instance of Output.\nThrough OutputParams, an Output receives its index (in case there’s multiple output processes) and a list of field indexes, in addition to the fields inherited from ComponentParams.\nOutputParams.Index indicates a unique index of the output process among the concurrent output processes generated by Baker. The procs configuration can be used to tune the total number of concurrent processes, see Pipeline configuration for details.\nNote that the output should extensively document in OutputDesc.Help if it is able to manage concurrent processing or if the user should set it with a single process (procs=1). Read Tuning concurrency for an in-depth guide to the subject.\nOutputParams.Fields is a list of FieldIndex that the output will receive, ordered as they are in the TOML. They’re also in the same order as the fields in OutputRecord.Fields, see below for details.\nIf, for any reason, the output needs to retrieve the fields name (like the SQLite output does to get the columns names), then OutputParams.FieldName can be used.\nOutput configuration and help The output configuration object (MyOutputConfig in the previous example) must export all configuration parameters that the user can set in the TOML topology file.\nEach field in the struct must include a help string tag (mandatory) and a required boolean tag (default to false).\nAll these parameters appear in the generated help. help should describe the parameter role and/or its possible values, required informs Baker it should refuse configurations in which that field is not defined.\nOutputRecord The OutputRecord channel received by the output component can be closed by Baker at any time and the output should return from the Run function as soon as possible when this happens.\nUntil that moment, the output component must continuously read new records, processing them.\nThe OutputRecord.Fields slice contains the string values of the fields that the user choose to send to the output configuring the fields key in the [output] section of the TOML topology file.\nFields are ordered in the same way than the slice of FieldIndex received in OutputParams.Fields.\nIn case of a raw output, OutputRecord.Record contains both the serialized record as a byte slice and the field values.\nPrepare data for uploading If the output component produces files on the local filesystem, then it should send their paths to the upload component (using the shared string channel), regardless of the real presence of a configured upload (that is unknown to the output). If the upload is absent, then Baker will ignore those messages.\nThe output can send a single message at the end of its job (think to a sqlite database that should only be uploaded before Baker exits) or can upload files periodically, like the FileWriter component does when it rotates (i.e. it stops writing to a file, send its path to the upload, and then creates a new file).\nWrite tests Tests for output components often require either mocking external resources/dependencies (think to an output writing to DynamoDB) or creating temporary files. How to test the components is strictly tied to the component implementation.\nFor these reasons there isn’t a single golden rule for testing outputs, but some common rules can be identified:\n test the New() (constructor-like) function, to check that the function is able to correctly instantiate the component with valid configurations and intercept wrong ones create small and isolated functions where possible and unit-test them test the whole component at integration level  The last point is where we can go a bit deeper. A possible strategy is to create a new output instance using the New function, passing it the in (from Baker to the component) and out (from the component to the upload) channels and use those channels to interact with the output.\nfunc TestMyOutput(t *testing.T) { cfg := ... // define cfg with component configuration  output := NewMyOutput(cfg) // use the contructor-like New function  outch := make(chan baker.OutputRecord) upch := make(chan string) wg := \u0026sync.WaitGroup{} wg.Add(1) go func() { outch \u003c- baker.OutputRecord{Fields: []string{\"a\", \"b\", \"c\"}, Record: []byte(\"rawrecord\")} // add more records to outch  close(outch) for upchpath := range upch { // check upchpath and set some vars/objs  if upchpath ... { // check the path or open the file or whatever...  checkVar = \"something\" } } wg.Done() }() // run the output, consuming the outch and sending results to upch  output.Run(outch, upch) close(upch) wg.Wait() // wait for the job to end  // now we can check the vars/objs created in the goroutine  if checkVar != wantVar { t.Fatalf(\"error!\") } } The SQLite component has a good example of this strategy.\n","excerpt":"Output components in Baker receive records at the end of the filter …","ref":"/docs/how-tos/create_output/","title":"Create a custom output component"},{"body":"The last (optional) component of a Baker pipeline is the Upload, whose job is to, precisely, upload local files produced by the output component.\nTo create an upload component and make it available to Baker, one must:\n Implement the Upload interface Fill an UploadDesc structure and register it within Baker via Components.  At the moment Baker only proposes a single Upload component, S3.\nThe Upload interface New Upload components need to implement the Upload interface.\ntype Upload interface { Run(upch \u003c-chan string) error Stop() Stats() UploadStats } The Run function implements the component logic, it is passed a channel from which the upload receives absolute paths of the files to upload. These files are produced by the Output component.\nUploadDesc var MyUploadDesc = baker.UploadDesc{ Name: \"MyUpload\", New: NewMyUpload, Config: \u0026MyUploadConfig{}, Help: \"High-level description of MyUpload\", } This object has a Name, that is used in the Baker configuration file to identify the upload, a constructor-like function (New), a config object (where the parsed upload configuration from the TOML file is stored) and a help text that must help the users to use the component and its configuration parameters.\nThe New function The New field in the UploadDesc object should be to assigned to a function that returns a new Upload.\nThe function receives an UploadParams object and returns an instance of Upload.\nIt should verify the configuration, accessed via UploadParams.DecodedConfig and initialize the component accordingly.\nUpload configuration and help The upload configuration object (MyUploadConfig in the previous example) must export all configuration parameters that the user can set in the TOML topology file.\nEach field in the struct must include a help string tag (mandatory) and a required boolean tag (default to false).\nAll these parameters appear in the generated help. help should describe the parameter role and/or its possible values, required informs Baker it should refuse configurations in which that field is not defined.\nThe files to upload Through the channel, the upload receives from the output paths to local files that it must upload.\nThe only Upload component implemented at the moment, S3, removes those files once uploaded, but there isn’t a golden rule for what to do with them. This is up to the upload component and should be chosen wisely and documented extensively.\nWrite tests Since, by definition, an upload component involves external resources, you either have to mock those resources or use them directly.\nSee an example of how to mock an external resource in the S3 upload.\nHowever writing a test that uses the actual external resource (a.k.a end-to-end testing) is out of the scope of this how-to.\nWe thereby provide some general suggestions to test the uploads:\n do not unit-test external libraries when possible, they should be already tested in their packages test the New() (constructor-like) function, to check that it is able to correctly instantiate the component with valid configurations and intercept wrong ones create small and isolated functions where possible and unit-test them test the whole component at integration level, either mocking the external resources or using a replica testing environment  The S3 upload component has good examples for both unit tests and integration tests.\n","excerpt":"The last (optional) component of a Baker pipeline is the Upload, whose …","ref":"/docs/how-tos/create_upload/","title":"Create a custom upload component"},{"body":"Baker can publish various kind of metrics that may be used to monitor a pipeline in execution. The metrics exported range from numbers giving an high-level overview of the ongoing pipeline (total processed records, current speed in records per second, etc.) or per-component metrics such as the number of files read or written, to performance statistics published by the Go runtime in order to monitor lower level information (objects, memory, garbage collection, etc.).\nAll components need to implement a Stats method where they can expose metrics. Baker calls the Stats method of each component once per second. Stats returns a predefined set of metrics (depending on the component type) and a baker.MetricsBag, in which one can add other metrics (of arbitrary name and type).\nLet’s illustrate this with metrics exported by a filter via baker.FilterStats:\ntype FilterStats struct { NumProcessedLines int64 NumFilteredLines int64 Metrics MetricsBag } In this case NumProcessedLines should represent the total number of processed lines since the filter creation, while NumFilteredLines is the number of discarded (i.e filtered) records. Due to historical reasons these fields have the word lines in them but they do mean the number of records.\nA practical example Let’s say our filter needs to perform HTTP requests in order to decide whether a record should be discarded, we might want to keep track of the requests' durations in an histogram. In this case, we would probably record a slice of time.Duration in our filter and call AddTimings on the returned MetricsBag.\nAn important point is that Baker may call Process and Stats concurrently, from different goroutines so you must use proper locking on data structures which are shared between the these two methods.\nfunc (f *myFilter) Process(r Record, next func(Record)) { atomic.AddInt64(\u0026myFilter.totalLines, 1) /* perform http request and keep track of its duration * in i.requestDurations */ if (/* filter logic*/) { // discard line  atomic.AddInt64(\u0026myFilter.filteredLines, 1) return } } func (i *myFilter) Stats() baker.FilterStats { i.mu.Lock() bag := make(baker.MetricsBag) bag.AddTimings(\"myfilter_http_request_duration\", i.requestDurations) i.mu.Unlock() return baker.FilterStats{ NumProcessedLines: atomic.LoadInt64(\u0026myFilter.totalLines), NumFilteredLines: atomic.LoadInt64(\u0026myFilter.filteredLines), Metrics: bag, } } Configuring metrics in TOML Baker configuration TOML files may have a [metrics] section dedicated to the configuration of a metrics client.\n[metrics.name] specifies the metrics client to use, from the list of all registered baker.MetricsClient. [metrics.config] specifies some configuration settings which are specific to the client you’re using.\nFor example, this is what the [metrics] section would look like with the Datadog metrics client:\n[metrics] name=\"datadog\" [metrics.config] host=\"localhost:8125\" # address of the dogstatsd client to which send metrics to prefix=\"myapp.baker.\" # prefix for all exported metric names send_logs=true # whether we should log messages (as Dogstatd events) or not  tags=[\"env:prod\", \"region:eu-west-1\"] # extra tags to associate to all exported metrics  Disabling metrics If you don’t want to publish any metrics, it’s enough to not provide the [metrics] TOML section in Baker configuration file.\nImplementing a new metrics client The metrics example shows an example implementation of baker.MetricsClient and how to register it within Baker so that it can be selected in the [metrics.name] TOML section.\nIn order to be selected from TOML, you must first register a baker.MetricsDesc instance within baker.Components.\nvar fooBarDesc = baker.MetricsDesc{ Name: \"MyMetrics\", Config: \u0026myyMetricsConfig{}, New: newMyMetrics, } where newMyMetrics is a constructor-like function receiving an interface{}, which is guaranteed to be of the type of the Config field value. This function should either return a ready to use baker.MetricsClient or an error saying why it can’t.\nfunc newMyMetrics(icfg interface{}) (baker.MetricsClient, error) Metrics.Client interface Once a baker.MetricsClient instance has been successfully created, it’s made available to and used by a Baker pipeline to report metrics. During construction, components receive the MetricsClient instance.\nbaker.MetricsClient supports the most common type of metric types: gauges, counters and histograms.\n","excerpt":"Baker can publish various kind of metrics that may be used to monitor …","ref":"/docs/how-tos/metrics/","title":"Export metrics"},{"body":"While running, Baker dumps statistics on stdout every second. This is an example line:\nStats: 1s[w:29425 r:29638] total[w:411300 r:454498 u:1831] speed[w:27420 r:30299] errors[p:0 i:0 f:0 o:0 u:0] The first bracket shows the current read and write speed, i.e. the records that entered the pipeline (reads) and the records that successfully exited from it (writes).\nThe second bracket is the total since the process was launched (the u: key is the number of files successfully uploaded).\nThe third bracket shows the average read/write speed (records per second).\nThe fourth bracket shows the number of records that were discarded at some point because of errors:\n p: records discarded for a parsing error i: input records discarded. Most of the time, this refers to validation issues. f: records discarded by filters. o: is the number of records that were discarded because of an error in the output component * u: is the number files whose upload has failed  * Notice that output components should be resilient to transient network failures, and they abort the process in case of permanent configuration errors, so the number here reflects records that could not be permanently written because eg. validation issues. Eg. think of an output that expects a column to be in a specific format, and rejects records where that field is not in the expected format. A real-world example is empty columns that are not accepted by DynamoDB.\n","excerpt":"While running, Baker dumps statistics on stdout every second. This is …","ref":"/docs/how-tos/read_stats/","title":"Read statistics"},{"body":"When a topology is configured with multiple output processes, Sharding allows to partition the records sent to each of them based on the value of a given field.\nSharding is enabled in the [output] section of the TOML file, by indicating the name of the field we wish to use to partition the records space.\nIn the following topology extract, we’re using a sharded Filewriter output and set the number of instances to 4 (i.e 4 shards). In our case, Baker is going to extract and hash the name field of each Record to determine which of the 4 Filewriter instances a Record is sent to:\n[input] ... [[filter]] ... [output] name=\"Filewriter\" sharding=\"name\" procs=4 [output.config] ... Limitations Baker only supports sharding at the output level. Baker implements other strategies so that other types of components (input, filters and uploads) maximize the pipeline performance.\nAlso, keep in mind that not all tasks can be parallelized, so not all outputs support sharding. So sharding is an intrinsic property that is only present on some Output components, but not all of them.\nOnly a single field can be used for sharding.\nHash functions The field selected for sharding must be “shardable”: in other words, a sharding function (or hash function) must be associated to that field.\nSince the aim of sharding is to uniformly distribute the load of incoming records between multiple instances of an output component, a good hash function should be uniform; in other words it should map as evenly as possible from the range of possible input values to the range of output values.\nThe range of output values is known, it is [0, MaxUint64] since in Baker hashes are uint64 values).\nHowever the range of possible input values depends on the domain. That’s where having knowledge of that particular domain will help in designing a hash function, that both guarantees the uniformity of output values with respect to input values, and in terms of performance.\nFor example, if you know the sharded field is only made of integers from 0 to 1000, the hash function would be implemented differently than if the values for that field are arbitrary long strings.\nIt’s however possible to use a non-optimal but best effort general hash function. (we’re planning to add this to Baker soon).\nA hash function should of course be deterministic (i.e the same input should always give the same output).\nRegister sharding functions The baker.Components structure links elements that may appear in the configuration, to the code eventually running when these elements are used inside a topology.\nSharding functions that may be used in topologies are stored inside of the ShardingFuncs field of baker.Components.\nShardingFuncs map[baker.FieldIndex]ShardingFunc And a ShardingFunc is a hash function that returns an uint64 for baker.Record\ntype ShardingFunc func(Record) uint64 Finally, filling ShardingFuncs is a matter of associating a shardable field to the sharding function that implements the hashing of that field.\nPutting it all together The following is an example of an hypothetical record schema with 3 fields named timestamp, city and country. Let’s say that we’d like to use timestamp and country for sharding but not city. We’re going to enable sharding on these two fields, but note that only one of them can be chosen for a given topology.\nThis is how implementing sharding for such a schema would look probably like:\nconst ( FieldTimestamp baker.FieldIndex = 0 // timestamp is unix epoch timestamp  FieldCity baker.FieldIndex = 1 // city name  FieldCountry baker.FieldIndex = 2 // 2 chars country code ) This is an hypothetical function to hash records based on the timestamp field which only contains integers:\nfunc hashTimestamp(r baker.Record) uint64 { // We know the timestamp is an integer, so we use that  // to efficiently compute a hash from it.  buf := r.Get(FieldTimestamp) ts, _ := strconv.Atoi(string(buf)) // Call super efficient integer hash function  return hashInt(ts) } And this is how hashing records based on a 2-char country code field would look like:\nfunc hashCountry(r baker.Record) uint64 { // We know the country is made of 2 characters, so we use that  // fact to efficiently compute a hash from it.  buf := r.Get(FieldCountry) country := buf[:2] // Call our super fast function that hashes 2 bytes.  return hash2bytes(country) } You can find here a full working example illustrating sharding in Baker.\n","excerpt":"When a topology is configured with multiple output processes, Sharding …","ref":"/docs/how-tos/sharding/","title":"Sharding setup"},{"body":"Baker allows to tune concurrency at various levels of a pipeline:\n input: Baker configuration doesn’t expose knobs to tune input concurrency as it highly depends on the input source and how the input is implemented filters: Baker runs N concurrent filter chains output: Baker runs M concurrent outputs  By default then, Baker processes records concurrently, without any guaranteed order.\nHowever, if you need to maintain the order of the records through the whole pipeline, it is still possible by disabling concurrency (see below for details).\nFilter chain concurrency The filter chain is a synchronous list of filters that are applied in the order in which they are listed in the topology TOML configuration file.\nBy default, though, Baker executes multiple concurrent filter chains (the default value is 16)\nFilterchain concurrency can be set defining the procs key in the [filterchain] section:\n[filterchain] procs=16 Setting the value to procs=1 disables the filter chain concurrency.\nConcurrent output The output concurrency can be set defining the procs key in the [output] section:\n[output] procs=32 The default value is 32.\nTo disable concurrency, set procs=1.\nOutput concurrency support For outputs that don’t support concurrency, procs=1 must be used to avoid corrupted output or lost data.\nRefer to the output documentation to know if it supports concurrent processing.\nWe’ll soon add a new function to the output to declare whether it supports concurrency, and Baker will return an error if procs\u003e1 is used with an output that doesn’t support it.  Guarantee Records order Although it’s not the primary goal of Baker, it is still possible to disable concurrency and thus guarantee records ordering from input to output.\nTo do so, add both procs=1 for output and filterchain, disabling concurrent processing for those components.\n","excerpt":"Baker allows to tune concurrency at various levels of a pipeline: …","ref":"/docs/how-tos/concurrency/","title":"Tuning concurrency"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_480734_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_480734_1920x1080_fill_q75_catmullrom_top.jpg); } }  Baker High performance modular pipelines for the Big Data era Documentation   Github repository           Blazing fast and open-source data processor  Baker pipelines can fetch, transform and store records in a flash  thanks to a high-performant and fully-parallel implementation.  Baker is open-source and written in Go by NextRoll, inc.         Modular pipeline Baker pipelines are assembled using fully modular components, including many AWS services, websocket, TCP, local disk, etc.\nCheck out the full list »    High performances Baker is fully parallel and maximizes the usage of both CPU-bound and I/O bound pipelines.\nPerformance examples »    Extendable With Baker it is super easy to write your own components and process any kind of structured data.\nHow to do it »        Contributions welcome! New components, Baker core or documentation. Please open a Pull-Request »\n   Found a bug? If you think you have found a problem, create a new issue »\n   Questions? Get in touch with the Baker team, join our Slack channel »\n    ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","title":"Baker, high performance modular pipelines for the Big Data era"},{"body":"","excerpt":"","ref":"/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"}]